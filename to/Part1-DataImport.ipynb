{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import bs4\n",
    "import pandas as pd\n",
    "import requests"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "response = requests.get(\"https://www.newsweek.com/newsfeed?page=1\", headers=headers)\n",
    "response"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "soup = bs4.BeautifulSoup(response.text)\n",
    "g_div = soup.find_all(\"article\", class_=\"l3\")\n",
    "g_div[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Find \"category\"\n",
    "cat = g_div[0].find(\"div\", class_=\"category\")\n",
    "\n",
    "#Get only the text\n",
    "cat.text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find url\n",
    "g_div[0].a['href']\n",
    "\n",
    "for i in g_div:\n",
    "    print( \"Found the URL:\", i.a['href'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the title of the article in the first box\n",
    "g_div[0].find(\"h3\").text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find the div with the \"summary\" class\n",
    "sum = g_div[0].find(\"div\", class_=\"summary\")\n",
    "sum.text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categories = []\n",
    "urls = []\n",
    "titles = []\n",
    "summaries = []\n",
    "\n",
    "for div in g_div:\n",
    "    try:\n",
    "        url = div.a['href']\n",
    "        urls.append(url)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cat = div.find(\"div\", class_=\"category\")\n",
    "        categories.append(cat.text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        title = div.find(\"h3\").text\n",
    "        titles.append(title)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        sum = div.find(\"div\", class_=\"summary\")\n",
    "        summaries.append(sum.text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame({\n",
    "    'Category':categories,\n",
    "    'URL':urls,\n",
    "    'Title':titles,\n",
    "    'Summary':summaries\n",
    "})\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Loop through 50 pages of listings and store category, URL, title, and summary in lists\n",
    "cat1=[]\n",
    "url1=[]\n",
    "title1=[]\n",
    "sum1=[]\n",
    "\n",
    "base_url = \"https://www.newsweek.com/newsfeed\"\n",
    "\n",
    "for i in range(1,51):\n",
    "    url = base_url+ \"?page=\" + str(i)\n",
    "    #print(url)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "    g_div = soup.find_all(\"article\", class_=\"l3\")\n",
    "    for div in g_div:\n",
    "        try:\n",
    "            url = div.a['href']\n",
    "            url1.append(url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            cat = div.find(\"div\", class_=\"category\")\n",
    "            cat1.append(cat.text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            title = div.find(\"h3\").text\n",
    "            title1.append(title)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            sum = div.find(\"div\", class_=\"summary\")\n",
    "            sum1.append(sum.text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame({\n",
    "    'Category':cat1,\n",
    "    'URL':url1,\n",
    "    'Title':title1,\n",
    "    'Summary':sum1\n",
    "})\n",
    "\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.to_csv(\"newsweek.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scraping one whole article"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the URL for the first article\n",
    "url_col = \"https://newsweek.com\" + df.iloc[1][1]\n",
    "url_col"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "resp = requests.get(url_col, headers=headers)\n",
    "soup1 = bs4.BeautifulSoup(resp.text)\n",
    "soup1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "h_div = soup1.find_all(\"div\", class_='article-body v_text paywall')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_p = soup1.find_all('p')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Join all tags together\n",
    "article = all_p[0].text + \" \"\n",
    "\n",
    "for i in range(1,len(all_p)):\n",
    "    article = article + all_p[i].text + \" \"\n",
    "\n",
    "article"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Scrape the articles found in the url column\n",
    "\n",
    "articles = []\n",
    "for i in range(0,len(df)):\n",
    "    url_col = \"https://newsweek.com\" + df.iloc[i][1]\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    resp = requests.get(url_col, headers=headers)\n",
    "    soup1 = bs4.BeautifulSoup(resp.text)\n",
    "    h_div = soup1.find_all(\"div\", class_='article-body v_text paywall')\n",
    "    all_p = soup1.find_all('p')\n",
    "    \n",
    "    article = all_p[0].text + \" \"\n",
    "\n",
    "    for i in range(1,len(all_p)):\n",
    "        try:\n",
    "            article = article + all_p[i].text + \" \"\n",
    "        except:\n",
    "            pass\n",
    "    articles.append(article)\n",
    "\n",
    "df['Full_text'] = articles"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.to_csv(\"newsweek2.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}